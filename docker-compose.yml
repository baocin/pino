version: '3.8'

# {
#   "nominatim-pino": "localhost:8080",
#   "timescaledb-pino": "localhost:5432",
#   "gotify-pino": "localhost:9090",
#   "realtime-pino": "localhost:8081",
#   "llamafile": "localhost:8082",
#   "whisper-streaming": "localhost:43007",
#   "openedai-vision": "localhost:5006"
# }

services:
  openedai-vision:
    build:
      context: ./openedai-vision
      dockerfile: Dockerfile
      args:
        - VERSION=latest
    image: ghcr.io/matatonic/openedai-vision
    env_file:
      - ./.env
    volumes:
      - ./openedai-vision/hf_home:/app/hf_home
      - ./openedai-vision/model_zoo:/app/model_zoo
      - ./openedai-vision/YanweiLi:/app/YanweiLi
      - ./openedai-vision/model_conf_tests.json:/app/model_conf_tests.json
    ports:
      - "5006:5006"
    networks:
      - pino-network
    command: ["python", "vision.py", "--model", "vikhyatk/moondream2"]

    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # florence2:
  #   container_name: florence2
  #   build:
  #     context: ./florence2
  #     dockerfile: Dockerfile
  #   ports:
  #     - "8010:8010"
  #   networks:
  #     - pino-network
    # real time is less important for ocr/image processing
    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  whisper-streaming:
    container_name: whisper-streaming
    build:
      context: ./whisper_streaming
      dockerfile: Dockerfile
    ports:
      - "43007:43007"
    networks:
      - pino-network
    healthcheck:
      test: ["CMD-SHELL", "pidof python3.11 || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 10
      
  scheduled-pino:
    container_name: scheduled-pino
    env_file:
      - ./.env
    build:
      context: .
      dockerfile: ./scheduled/Dockerfile
    volumes:
      - ./scheduled:/app/scheduled
    depends_on:
      timescaledb-pino:
        condition: service_healthy
    networks:
      - pino-network
    healthcheck:
      test: ["CMD-SHELL", "pidof python3.11 || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 10

  realtime-pino:
    container_name: realtime-pino
    env_file:
      - ./.env
    build:
      context: .
      dockerfile: ./realtime/Dockerfile
    volumes:
      - ./realtime:/app/realtime
    depends_on:
      timescaledb-pino:
        condition: service_healthy
      gotify-pino:
        condition: service_healthy
      whisper-streaming:
        condition: service_healthy
    networks:
      - pino-network
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; import sys; sys.exit(0 if urllib.request.urlopen('http://localhost:${REALTIME_SERVER_PORT}/heartbeat').getcode() == 200 else 1)\""]
      interval: 5s
      timeout: 5s
      retries: 10
    # runtime: nvidia
    ports:
      - "${REALTIME_SERVER_PORT}:${REALTIME_SERVER_PORT}"
    # All gpu based processing will be ousourced to other containers so dependencies are easier to manage
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  llamafile:
    container_name: llamafile-Meta-Llama-3.1-8B.Q4_0
    image: iverly/llamafile-docker:latest
    ports:
      - "8082:8080"
    volumes:
      - ./llamafile/model/Meta-Llama-3.1-8B.Q4_0.llamafile:/model
    networks:
      - pino-network
    # -ngl 9999 to enable GPU offloading

  whisper-streaming:
    container_name: whisper-streaming
    build:
      context: ./whisper_streaming
      dockerfile: Dockerfile
    ports:
      - "43007:43007"
    networks:
      - pino-network
    healthcheck:
      # test: ["CMD-SHELL", "python3 -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); result = s.connect_ex(('localhost', 43007)); s.close(); exit(0 if result == 0 else 1)\""]
      test: ["CMD-SHELL", "exit 0"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 15s # wait 15 seconds for the warm up, don't actually care about the health

    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  openedai-vision:
    build:
      context: ./openedai-vision
      dockerfile: Dockerfile
      args:
        - VERSION=latest
    image: ghcr.io/matatonic/openedai-vision
    env_file:
      - ./.env
    volumes:
      - ./openedai-vision/hf_home:/app/hf_home
      - ./openedai-vision/model_zoo:/app/model_zoo
      - ./openedai-vision/YanweiLi:/app/YanweiLi
      - ./openedai-vision/model_conf_tests.json:/app/model_conf_tests.json
    ports:
      - "5006:5006"
    networks:
      - pino-network
    command: ["python", "vision.py", "--model", "vikhyatk/moondream2"]

volumes:
  nominatim-data:
    driver: local
  timescaledb-data:
    driver: local
  gotify-data:
    driver: local

networks:
  pino-network:
    driver: bridge
